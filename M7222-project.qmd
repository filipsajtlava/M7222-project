---
title: "Infrared Thermography Data"
author: "Filip Šajtlava, Klára Čevelová, Barbora Vařeková"
date: "today"
format: 
    html:
        toc: true
        code-fold: true
        code-tools:
            source: true
            toggle: true
            caption: "Show the code"
        code-summary: "Show the code"
        css: styles.css
theme:
    light: flatly
#highlight-style: github
---

```{r}
#| include: false
library(ggplot2) |> suppressPackageStartupMessages()
library(dplyr) |> suppressPackageStartupMessages()
library(knitr) |> suppressPackageStartupMessages()
library(ggcorrplot) |> suppressPackageStartupMessages()
library(viridis) |> suppressPackageStartupMessages()
library(kableExtra) |> suppressPackageStartupMessages()
library(factoextra) |> suppressPackageStartupMessages()
library(FactoMineR) |> suppressPackageStartupMessages()
library(factoextra) |> suppressPackageStartupMessages()
library(car) |> suppressPackageStartupMessages()
library(fastDummies) |> suppressPackageStartupMessages()
library(statmod) |> suppressPackageStartupMessages()
library(boot) |> suppressPackageStartupMessages()

df <- read.csv("infrared_train.csv")
vir_col <- viridis(30, option = "magma")
```

# 1 Preprocessing

We are going to be working with the dataset `infrared_train.csv` with the following dimensions, modelling the variable $aveOralM$:

```{r}
#| layout-ncol: 1
#| fig-cap:
#|  - "Histogram of $aveOralM$"
dim(df)

df |> ggplot(aes(x = aveOralM, y = after_stat(density))) +
    geom_histogram(color = "black", fill = vir_col[20], bins = 30) +
    geom_density(color = vir_col[25], lwd=1.5, bw = 0.1, alpha = 0.75) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Visibly right-skewed, we can check the Q-Q plot and Kolmogorov-Smirnov test for normality:

```{r, warning=FALSE}
#| layout-ncol: 1
#| fig-cap:
#|  - "Q-Q plot"

df |> ggplot(aes(sample = aveOralM)) +
    stat_qq(color = vir_col[8]) +
    stat_qq_line(color = vir_col[14], lwd = 1) + 
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

aveOralM_scaled <- df |> 
    mutate(aveOralM_scaled = (aveOralM - mean(aveOralM)) / sqrt(var(aveOralM))) |>
    pull(aveOralM_scaled)

aveOralM_scaled |> ks.test("pnorm")

```

The test and the Q-Q plot confirm our suspicion of the data being non-gaussian.

Next let's take a look at the basic structure:

```{r}
#| layout-ncol: 1

df |>
    select(c(Gender, Age, Ethnicity, Humidity, Distance, aveOralM)) |>
    head(5) |>
    kable()

```

And at the original categorical variables:

::: {.grid}

::: {.g-col-2}
:::

::: {.g-col-8}

```{r}
character_variables <- df |>
    select(where(is.character)) |>
    colnames()

table(df[, character_variables[1]]) |> t() |> kable()
```

:::

::: {.g-col-2}
:::

:::

```{r}
table(df[, character_variables[2]]) |> t() |> kable()
table(df[, character_variables[3]]) |> t() |> kable()
```

Some of the $Gender$ observations are incorrectly encoded. On top of that, the $Age$ variable contains ambiguous observations within the group of 21-30 year olds. We are going to group those six people with the younger ones based on the empirical probability of them belonging to this group.

The next problem are the sparse amounts of older people that might prove difficult for the model when attempting to correctly estimate the effect of this group (the same goes for American Indian or Alaskan Natives).

```{r}

categorical_fix <- function(df) {
    # Gender fix
    df <- df |>
        mutate(Gender = case_when(
            Gender %in% c("Female", "female") ~ "F",
            Gender %in% c("Male", "male") ~ "M"
            )
        )

    df$Gender <- factor(df$Gender)

    # Age fix
    df <- df |>
        mutate(Age = case_when(
            Age %in% c(">60", "31-40", "41-50", "51-60") ~ "31+",
            Age == "21-30" ~ "21-25",
            TRUE ~ Age
            )
        )

    # Ethnicity - leave it out for now

    df <- df |>
        mutate(Ethnicity = case_when(
            Ethnicity == "American Indian or Alaskan Native" ~ "Multiracial",
            TRUE ~ Ethnicity
            )
        )

    df$Ethnicity <- factor(df$Ethnicity)

    return(df)
}

df <- categorical_fix(df)
```


## 1.1 Missing Values

We should check the dataset for any <u>NA values</u> and the total sum of observations (rows) containing any empty values:

::: {.grid}

::: {.g-col-1}
:::

::: {.g-col-4}

```{r}
df |>
    is.na() |>
    colSums() |>
    as.data.frame() |>
    rename("Rows with NAs" = "colSums(is.na(df))") |>
    filter(`Rows with NAs` > 0) |> kable()
```
:::

::: {.g-col-2}
:::

::: {.g-col-4}
```{r}

is.na(df) |> 
    table() |> 
    as.data.frame() |> 
    rename("Missing values" = Var1) |> kable()
```

$\rightarrow$ Implies NA values are unique for each row


:::

::: {.g-col-1}
:::

:::

Here are the observations with age as a missing value:

```{r}
df |> 
    filter(if_any("Age", is.na)) |> 
    kable()
```


There are 3 possibilities:

1. Delete all of the NA rows because of their small proportion (~5 %) in the dataset,
2. Impute the values using means, medians, most occurring values, etc.,
3. Investigate if the emptiness doesn't have a deeper meaning (not very probable because of their overall small frequency).

### 1.1.1 Imputation of $Age$ and $Distance$

We will impute the rows for $Age$ and $Distance$. First we have to take a look at the overall structure using `summary()`:

::: {.grid}

::: {.g-col-2}
:::

::: {.g-col-3}

```{r}
df |> select(Distance) |> summary()
```

:::

::: {.g-col-2}
:::

::: {.g-col-4}
```{r}
df |> select(Age) |> table() |> t() |> kable()
```

:::

::: {.g-col-1}
:::

:::

As we can see, one of the observations ($Distance = 79$) falls entirely outside its feasible range. We will, alongside with other faulty values, remove it later in the data-cleaning process.

```{r}
#| layout-ncol: 2
#| fig-cap:
#|  - "Histogram of $Distance$"
#|  - "Barplot of $Age$"

df |> 
    filter(Distance < 10) |> 
    ggplot(aes(x = Distance, y = after_stat(density))) +
    geom_histogram(fill = vir_col[8], color = "black", binwidth = 0.05) +
    geom_density(color = vir_col[14], lwd = 1.5, bw = 0.02) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

df |>
    ggplot(aes(x = Age)) +
    geom_bar(fill = vir_col[17], color = "black") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Due to the observed skewness we will impute $Distance$ with median. For the $Age$ variable, the most occurring value will be used (that being the cohort of 18-20 year olds).

```{r}

distance_no_extremes <- df |>
    filter(Distance < 10 & !is.na(Distance)) |>
    pull(Distance)

most_occuring_age <- df |> 
    select(Age) |> 
    table() |> 
    as.data.frame() |> 
    arrange(desc(Freq)) |>
    pull(Age) %>%
    .[1] |>
    as.character()

df <- df |> 
    mutate(
        Distance = ifelse(
            Distance > 10 | is.na(Distance),
            median(distance_no_extremes),
            Distance
        ),
        Age = ifelse(
            is.na(Age),
            most_occuring_age,
            Age
        )
    )

df$Age <- factor(df$Age)
```

### 1.1.2 Imputation of $aveAllR13\_1$ and $canthiMax1$

```{r, warning=FALSE}
#| layout-ncol: 2
#| fig-cap:
#|  - "Histogram of $aveAllR13\_1$"
#|  - "Histogram of $canthiMax1$"

df |>
    ggplot(aes(x = aveAllR13_1, y = after_stat(density))) +
    geom_histogram(fill = vir_col[20], color = "black", bins = 30) +
    geom_density(color = vir_col[24], lwd = 1.5) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

df |>
    ggplot(aes(x = canthiMax1, y = after_stat(density))) +
    geom_histogram(fill = vir_col[27], color = "black", bins = 30) +
    geom_density(color = vir_col[30], lwd = 1.5) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Due to these variables having more missing observations, we should impute taking into consideration other relationships they might have.

```{r, warning=FALSE}
#| layout-ncol: 2
#| fig-cap:
#|  - "Histograms of $aveAllR13\_1$ based on $Gender$"
#|  - "Boxplots of $aveAllR13\_1$ based on $Age$"
df |>
    ggplot() +
    geom_histogram(aes(x = aveAllR13_1, y = after_stat(density), fill = Gender, alpha = Gender), color = "black", bins = 30, position = "identity") + 
    geom_density(aes(x = aveAllR13_1, color = Gender), lwd = 1.5) +

    scale_color_manual(values = c("F" = vir_col[22], "M" = vir_col[9])) +
    scale_alpha_manual(values = c("F" = 1, "M" = 0.75)) +
    scale_fill_manual(values = c("F" = vir_col[20], "M" = vir_col[7])) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))


df |>
    ggplot() +
    geom_boxplot(aes(y = aveAllR13_1, fill = Age)) +
    scale_fill_manual(values = c("18-20" = vir_col[26], 
    "21-25" = vir_col[20], "26-30" = vir_col[12], "31+" = vir_col[5])) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Males do seem to be a bit more right-skewed with the values, implying their values of $aveAllR13\_1$ are higher. On the boxplot, it is clear that aging has a negative effect on the variable, <span style="color: #3F0F72;">though it is also important to realize that the classes have varying counts of observations, with the last two groups being only a small fraction of the first ones.</span>

We can also take a look at the modelled variable scatterplot in combination with the $aveAllR13\_1$ to find out their positive correlation:

```{r, warning=FALSE}
#| layout-ncol: 1
#| fig-cap:
#| - "Scatterplot of $aveAllR13\_1$ in relation to the modelled variable"
df |>  
    ggplot(aes(x = aveAllR13_1, y = aveOralM, color = Gender)) +
    geom_point(cex = 1.65) + 
    scale_color_manual(values = c("F" = vir_col[20], "M" = vir_col[5])) +
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Using a linear model we estimated the trend of both sexes, with males being more positively correlated to the outcome on the basis of $aveAllR13\_1$.

We are also able to see some extreme values, with two people having internal oral temperature of more than $40 ^\circ C$, which is possible, but highly unlikely.

```{r, warning=FALSE}
#| fig-cap:
#| - "Scatterplot of $aveAllR13\_1$ in relation to the modelled variable"
df |>  
    ggplot() +
    geom_point(aes(x = aveAllR13_1, y = aveOralM, color = Age),cex = 1.65) + 
    scale_color_manual(values = c("18-20" = vir_col[26], 
    "21-25" = vir_col[20], "26-30" = vir_col[12], "31+" = vir_col[5])) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

Based on these observations, we will impute the values using the medians of $aveAllR13\_1$ based on their $Gender$.

::: {.grid}

::: {.g-col-4}
:::

::: {.g-col-4}

```{r}
df |>
    filter(!is.na(aveAllR13_1)) |>
    group_by(Gender) |>
    summarise(
        mean = round(mean(aveAllR13_1), 3),
        median = round(median(aveAllR13_1), 3)
    ) |> kable()
```

:::

::: {.g-col-4}
:::

:::

```{r, warning=FALSE}
#| fig-cap:
#| - "Scatterplot of $aveAllR13\_1$ in relation to the modelled variable"

medians_aveAllR13_1 <- df |>
    group_by(Gender) |>
    summarise(
        median_value = median(aveAllR13_1, na.rm = TRUE)
    )

female_median_aveAllR13_1 <- medians_aveAllR13_1 |> 
    filter(Gender == "F") |> pull(median_value)
male_median_aveAllR13_1 <- medians_aveAllR13_1 |> 
    filter(Gender == "M") |> pull(median_value)

df <- df |>
    mutate(
        aveAllR13_1 = ifelse(
            is.na(aveAllR13_1),
            ifelse(
                Gender == "M",
                male_median_aveAllR13_1,
                female_median_aveAllR13_1
            ),
            aveAllR13_1
        )
    )
```

The variable $canthiMax1$ will be imputed in a similar fashion (taking into account that even 23 missing values still make up only a measly 3 % of the entire dataset).

```{r, warning=FALSE}
#| layout-ncol: 2
#| fig-cap:
#| - "Boxplots of $canthiMax1$ based on $Gender$"
#| - "Scatterplot of $canthiMax1$ in relation to the modelled variable"
df |>
    ggplot() +
    geom_boxplot(aes(y = canthiMax1, fill = Gender)) +
    scale_fill_manual(values = c("F" = vir_col[20], "M" = vir_col[5])) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

df |>
    ggplot() + 
    geom_point(aes(x = canthiMax1, y = aveOralM, color = Age),cex = 1.65) + 
    scale_color_manual(values = c("18-20" = vir_col[26], 
    "21-25" = vir_col[20], "26-30" = vir_col[12], "31+" = vir_col[5])) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

The variable $canthiMax1$ is a bit more right-skewed, so using median would be a wiser choice compared to the mean.

::: {.grid}

::: {.g-col-4}
:::

::: {.g-col-4}

```{r}
df |>
    filter(!is.na(canthiMax1)) |>
    group_by(Gender) |>
    summarise(
        mean = round(mean(canthiMax1), 3),
        median = round(median(canthiMax1), 3)
    ) |> kable()
```

:::

::: {.g-col-4}
:::

:::

```{r}
medians_canthiMax1 <- df |>
    group_by(Gender) |>
    summarise(
        median_value = median(canthiMax1, na.rm = TRUE)
    )

female_median_canthiMax1 <- medians_canthiMax1 |> 
    filter(Gender == "F") |> pull(median_value)
male_median_canthiMax1 <- medians_canthiMax1 |> 
    filter(Gender == "M") |> pull(median_value)

df <- df |>
    mutate(
        canthiMax1 = ifelse(
            is.na(canthiMax1),
            ifelse(
                Gender == "M",
                male_median_canthiMax1,
                female_median_canthiMax1
            ),
            canthiMax1
        )
    )
```

## 1.2 Error Values

We have already found some outliers in the variable $Distance$. Now we have to take a look at other variables possibly being encoded wrongly.

```{r, warning=FALSE}
#| layout-ncol: 3
#| fig-cap:
#| - "$T\_LC\_Wet1$ histogram"
#| - "$T\_FHTC1$ histogram"
#| - "$T\_FHC\_Max1$ histogram"

df |> ggplot() +
    geom_histogram(aes(x = T_LC_Wet1), color = "black", fill = vir_col[8], bins = 30) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

df |> ggplot() +
    geom_histogram(aes(x = T_FHTC1), color = "black", fill = vir_col[13], bins = 30) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

df |> ggplot() +
    geom_histogram(aes(x = T_FHC_Max1), color = "black", fill = vir_col[18], bins = 30) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

The histograms show big clusters of observations around a specific interval, with only a few lying far away. These outlying points can almost certainly be classified as error terms - for example $T\_FHC\_Max1$ = $100^\circ C$ indicating boiling temperatures measured on the forehead. 

To be fully confident in the validity of our data, we will remove all observations of $Distance$, $T\_LC\_Wet1$ and $T\_FHC\_Max1$ without attempting to impute them, since there is a significant issue with how they were originally obtained.

In addition, since the $T\_FHTC1$ variable contains nine observations with a value of 0, discarding them would be unnecessary; they can be reasonably imputed using the mean of the other average temperature measurements from the different forehead locations (as they are all non-zero).

```{r}

df <- df |> 
    filter(T_LC_Wet1 > 25 & T_LC_Wet1 < 40)

df <- df |>
    rowwise() |>
    mutate(T_FHTC1 = ifelse(
        T_FHTC1 == 0,
        mean(c(T_FHCC1, T_FHRC1, T_FHLC1, T_FHBC1)),
        T_FHTC1
        ) 
    ) |>
    ungroup() |>
    as.data.frame()

df <- df |>
    filter(T_FHC_Max1 < 40)
```

# 2 Visualization

Before selecting a model, we should examine the structure of the variables themselves, paying attention to potential multicollinearity, which can inflate standard errors and lead to incorrect confidence intervals of our predicted coefficients $\widehat{\beta}$.

```{r, warning=FALSE}
#| fig-width: 8
#| fig-height: 8
#| warning: FALSE
#| fig-cap:
#|  - "Correlation matrix of continuous variables"
cor_matrix <- df |>
    select(where(is.numeric)) |>
    cor() |>
    round(1)

cor_matrix |> 
    ggcorrplot(
    type="lower",
    lab=TRUE,
    lab_size=3,
    lab_col="black",
    outline.color = "black"
    ) +
    scale_fill_viridis_c(
        option="magma",
        limits=c(-1, 1)
    ) +
    theme_gray() +
    labs(x = NULL, y = NULL) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA),
    axis.text.x = element_text(angle = 45, hjust = 1))
```

$aveOralM$ is in the right-most column of the matrix. <span style="color: #3F0F72;">The correlations in the plot are rounded to the nearest decimal</span>, meaning that they aren't actually zeros or ones.

```{r, warning=FALSE}
#| fig-cap:
#|  - "PCA plot of 2 highest variance dimensions"

numeric_variables <- df |>
    select(-aveOralM) |>
    select(where(is.numeric)) |>
    colnames()

pca_result <- df |>
    select(all_of(numeric_variables)) |>
    prcomp(center = TRUE, scale. = TRUE)

pca_result |> fviz_pca_biplot(
    geom.ind = "point",
    col.ind = "cos2",
    gradient.cols = vir_col,
    pointsize = 1.5,
    repel = TRUE,
    geom.var = c("text", "arrow"),
    col.var = "black",
    legend.title = list(color = "Explained by dim")
    ) +
    theme_gray() +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA)) +
    labs(title = NULL)
```

This PCA plot shows that ~$80 \%$ of the variance is explained by these two dimensions. The arrows form two disctinct 'clusters', representing mainly the canthus area of the face (lower arrows) and the forehead area (higher arrows).

The most straightforward interpretation of the first dimension is the overall facial temperature, with the canthus-related variables accounting for a slightly larger share of the variance. The second dimension appears to capture the contrast or difference between the two specific facial regions.

The ambient temperature also had a strong effect on the facial temperature, affecting mainly the forehead area. On the other hand, the distance variable had a small negative effect on the temperature, with the correlation matrix confirming this effect.

```{r}
#| fig-cap:
#|  - "PCA without the observations"
pca_result |> 
fviz_pca_biplot(
    geom.ind = "none",
    repel = TRUE,
    geom.var = c("text", "arrow"),
    col.var = "black",
    labelsize = 4,
) +
    theme_gray() +
    labs(title = NULL) + 
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

The last plot showcases the MCA of categorical variables, where points closer to the center (0,0) can be interpreted as more 'ordinary' or expected, whereas categories such as <b>Multiracial</b>, <b>31+</b> and <b>26-30</b> deviate more noticeably from the rest.

<span style="color: #3F0F72;"> It's important to note that only a very small portion of the overall variance is explained, so the results should be interpreted with caution.</span>

```{r}
#| fig-cap:
#|  - "Correspondence analysis of categorical variables"

active_vars <- df |>
    select(where(is.factor))

res.mca <- MCA(
  active_vars,
  graph = FALSE
)

res.mca |> fviz_mca_biplot(
    repel = TRUE,
    geom.ind = "none",
    col.var = "name",
    pointsize = 5,
    labelsize = 4,
    palette = c(vir_col[7], vir_col[7], vir_col[7], vir_col[7], 
                vir_col[14], vir_col[14], vir_col[21], vir_col[14], vir_col[21], 
                vir_col[14], vir_col[14])
    ) + 
    theme_gray() + 
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA),
    legend.position = "none") +
    labs(title = NULL)
```

# 3 Modeling

First, we will fit a simple linear model using all the existing variables. Next, we will fit a Gamma GLM with a log link for easier interpretability (the canonical link for Gamma is the inverse). We chose this model because the distribution of $aveOralM$ is slightly right-skewed and contains only positive values.

Given the high values in the correlation matrix and the clustering of variables in the PCA plot, models like these will almost certainly suffer from multicollinearity.

```{r}
full_lm <- lm(data = df, formula = aveOralM ~ .)
full_glm_gamma <- glm(data = df, formula = aveOralM ~ ., family = Gamma(link = "log"))
vif(full_lm) |> head(11) |> t() |> kable()
```

The model clearly suffers from multicollinearity and is therefore unusable. The Gamma GLM exhibits the same issue, overfitting the data and reaching an AIC of approximately $−4500$. We can take a look at the plot of the fitted values to residuals (the deviance residuals for GLM).

```{r, message=FALSE}
#| layout-ncol: 2
#| fig-cap:
#| - "Fitted values to residuals Normal, canonical link"
#| - "Fitted values to deviance residuals Gamma, log link"
models_df <- data.frame(
    fitted_lm = full_lm$fitted.values,
    residuals_lm = full_lm$residuals,
    fitted_glm = full_glm_gamma$fitted.values,
    residuals_glm_dev = resid(full_glm_gamma, type = "deviance")
)

models_df |>
    ggplot(aes(x = fitted_lm, y = residuals_lm)) +
    geom_point(color = vir_col[10]) +
    geom_smooth(formula = y ~ x, se = FALSE, color = vir_col[5]) +
    geom_hline(yintercept = 0, linetype = "dashed", color = vir_col[10]) +
    labs(x = "Fitted values", y = "Residuals") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

models_df |>
    ggplot(aes(x = fitted_glm, y = residuals_glm_dev)) +
    geom_point(color = vir_col[20]) +
    geom_smooth(formula = y ~ x, se = FALSE, color = vir_col[15]) +
    geom_hline(yintercept = 0, linetype = "dashed", color = vir_col[10]) +
    labs(x = "Fitted values", y = "Deviance residuals") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))
```

We observed that the results of the two models are highly comparable, suggesting that they capture the behavior of the variable in a similar way.

From this point onwards, we will focus on the Gamma GLM. An important first step is to one-hot encode the categorical variables (create dummy variables), which allows us to freely choose reference levels and remove specific categories if we deem it appropriate.

## 3.1 Model Selection 

We chose the model:
$$
\begin{aligned}
\log(\mu) = \beta_0 &+ \beta_1 \cdot Gender_{Female} \\
&+ \beta_2 \cdot Age_{18-20} + \dots + \beta_4 \cdot Age_{31+} \\
&+ \beta_5 \cdot Ethnicity_{Asian} + \dots + \beta_8 \cdot Ethnicity_{Multiracial} \\
&+ \beta_9 \cdot T_{atm} + \beta_{10} \cdot Humidity + \beta_{11} \cdot Distance \\ 
&+ \beta_{12} \cdot T_{OR1} + \beta_{13} \cdot T_{Max1} + \beta_{14} \cdot T_{FHCC1} + \beta_{15} \cdot canthi4Max1,
\end{aligned}
$$

as we aimed to incorporate all categorical variables, we chose male gender, the 21–25 age cohort, and white ethnicity as the reference categories. Next, we added extraneous factors such as ambient temperature, humidity, and distance. Finally, we included the most important variables - the actual temperatures. Our selection was guided by the correlation matrix and PCA, which helped to identify the most important clusters without including too many variables - preserving interpretability and avoiding multicollinearity.

```{r}
#| echo: false

table <- data.frame(
  Variable = c("$T_{OR1}$", "$T_{Max1}$", "$T_{FHCC1}$", "$canthi4Max1$"),
  Description = c("Average temperature of the mouth region", 
                  "Maximum temperature of the entire face", 
                  "Average temperature within the centre of the forehead", 
                  "Average temperature within the canthi area")
)

kable(table, escape = FALSE)
```

Our approach was to focus on the average temperatures, as they are less affected by extreme values, while also accounting for the maximum values through 
$T_{Max1}$. The selected average temperatures represent the three most important and distinguishable regions of the face: the mouth region, the forehead, and the area in between. On top of that, they also represent the 4 most correlated variables to the modelled variable.

```{r}
df_dummies <- df
df_dummies <- dummy_cols(df_dummies, select_columns = "Ethnicity", remove_first_dummy = FALSE)
df_dummies <- dummy_cols(df_dummies, select_columns = "Age", remove_first_dummy = FALSE)
df_dummies <- dummy_cols(df_dummies, select_columns = "Gender", remove_first_dummy = FALSE)
df_dummies <- df_dummies |>
    rename(
        Ethnicity_Black = `Ethnicity_Black or African-American`,
        Ethnicity_Hispanic = `Ethnicity_Hispanic/Latino`
    )

optim_glm_gamma <- glm(data = df_dummies, 
    formula = aveOralM ~ Gender_F + `Age_18-20` + `Age_26-30` + `Age_31+` + 
    Ethnicity_Asian + Ethnicity_Black + Ethnicity_Hispanic + Ethnicity_Multiracial + 
    T_atm + Humidity + Distance + T_OR1 + T_Max1 + T_FHCC1 + canthi4Max1, 
    family = Gamma(link = "log"))

vif(optim_glm_gamma) |> t() |> kable()
summary(optim_glm_gamma)
```

The summary shows many statistically insignificant variables. We will remove them from the model step by step, starting with those with the highest p-values. The model's AIC is now a more realistic $540.9$, which we will attempt to further reduce.

```{r}
optim_glm_gamma <- glm(
    data = df_dummies, 
    formula = aveOralM ~ Ethnicity_Asian + Ethnicity_Black + T_atm + Distance + T_OR1 + T_Max1 + canthi4Max1, 
    family = Gamma(link = "log")
    )

summary(optim_glm_gamma)
```

The model has become much simpler, and the AIC has improved slightly. It is important to check for high-leverage extreme values that could be distorting the model. Cook’s distance can be used for this purpose.

```{r, message=FALSE}
#| layout-ncol: 2
#| fig-cap:
#| - "Cook's distance plot"
#| - "Q-Q plot of quantile residuals" 
cooks_df <- data.frame(
    index = 1:length(cooks.distance(optim_glm_gamma)),
    cooks_distance = cooks.distance(optim_glm_gamma),
    qq_resid = qresid(optim_glm_gamma),
    theoretical_q = qqnorm(qresid(optim_glm_gamma), plot.it = FALSE)$x
)
cook_threshold <- 8 / (dim(df)[1] - 2 * (dim(df)[2] - 1))

cooks_df |>
    ggplot(aes(x = index, y = cooks_distance)) +
    geom_point(color = vir_col[10]) +
    geom_hline(yintercept = cook_threshold, linetype = "dotdash", color = vir_col[15], linewidth = 0.8) +
    geom_point(data = subset(cooks_df, cooks_distance > cook_threshold), 
               color = vir_col[20], size = 3, shape = 21, fill = vir_col[20], alpha = 0.6) +
    geom_text(data = subset(cooks_df, cooks_distance > cook_threshold), aes(label = index), vjust = -1, hjust = 1) +
    labs(
        x = "Observation Index",
        y = "Cook's Distance"
    ) +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA)) 

cooks_df |>
    ggplot() +
    geom_text(
    data = subset(cooks_df, cooks_distance > cook_threshold),
    aes(x = theoretical_q, y = qq_resid, label = index),
    vjust = -1, hjust = 1
    ) +
    stat_qq_line(aes(sample = qq_resid), color = vir_col[15], lwd = 1) +
    stat_qq(aes(sample = qq_resid), color = vir_col[10]) +
    geom_point(data = subset(cooks_df, cooks_distance > cook_threshold),
    aes(x = theoretical_q, y = qq_resid), 
    color = vir_col[20], size = 3,
    fill = vir_col[20], alpha = 0.6) +
    theme(panel.border = element_rect(color = "black", linewidth = 1, fill = NA))

```

We highlighted the observations with sufficiently high Cook’s distance and plotted the quantized residuals on a Q–Q plot. The results clearly indicate the presence of high-leverage points that are degrading the overall quality of the model.

Based on this observation, we added a dummy variable to account for the extreme values in the model. If its coefficient turns out to be statistically significant, it would indicate that these observations exert a specific influence on the response that the original model was unable to capture.

```{r}
df_dummies$high_influence <- cooks_df$cooks_distance > cook_threshold
optim_outliers_glm_gamma <- glm(
    data = df_dummies, 
    formula = aveOralM ~ Ethnicity_Asian + T_atm + Distance + T_OR1 + T_Max1 + canthi4Max1 + high_influence, 
    family = Gamma(link = "log"))

summary(optim_outliers_glm_gamma)
```

The coefficient is highly statistically significant, indicating that these outliers do carry an additional information that the model cannot see.
Unfortunately, we did not find a cleaner solution than removing those specific observations, allowing the model to focus on the relationships it is actually able to capture.

```{r}
df_dummies_no_out <- df_dummies |>
    filter(!(cooks_df$cooks_distance > cook_threshold))

optim_no_out_glm_gamma <- glm(
    formula = aveOralM ~ Ethnicity_Asian + Ethnicity_Black + T_atm + T_Max1 + T_OR1 + canthi4Max1,
    family = Gamma(link = "log"),
    data = df_dummies_no_out
)

summary(optim_no_out_glm_gamma)
```

After fitting the model on the new data and removing the insignificant covariates, the AIC has gotten significantly better, more than halving it's entire value to $152.25$. 

```{r, message=FALSE}
#| layout-ncol: 2
#| fig-cap:
#| - "Fitted values to deviance residuals Gamma, log link"
#| - "With interaction term"

models_no_out_df = data.frame(
    optim_fitted_glm = optim_no_out_glm_gamma$fitted.values,
    optim_residuals_glm_dev = resid(optim_no_out_glm_gamma, type = "deviance")
)

models_no_out_df |>
    ggplot(aes(x = optim_fitted_glm, y = optim_residuals_glm_dev)) +
    geom_point(color = vir_col[10]) +
    geom_smooth(formula = y ~ x, se = FALSE, color = vir_col[5]) +
    geom_hline(yintercept = 0, linetype = "dashed", color = vir_col[5]) +
    labs(x = "Fitted values", y = "Deviance residuals") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

mean_T_OR1 <- mean(df_dummies_no_out$T_OR1)
mean_canthi4Max1 <- mean(df_dummies_no_out$canthi4Max1)
df_dummies_no_out <- df_dummies_no_out |>
    mutate(
        T_OR1_c = T_OR1 - mean_T_OR1,
        canthi4Max1_c = canthi4Max1 - mean_canthi4Max1
    ) |>
    slice(-600) # After fitting the model and checking for further problmes this observation is absolutely destroying the model, with 20-times the cook's distance value of the next highest, also distorting the dev. residuals to fitted  

optim_nonlin_no_out_glm_gamma <- glm(
    formula = aveOralM ~ Ethnicity_Asian + T_atm + T_Max1 +
              T_OR1_c * canthi4Max1_c,
    family = Gamma(link = "log"),
    data = df_dummies_no_out
)

models_no_out_nonlin_df = data.frame(
    optim_fitted_glm = 
        optim_nonlin_no_out_glm_gamma$fitted.values,
    optim_residuals_glm_dev = 
        resid(optim_nonlin_no_out_glm_gamma, type = "deviance")
)

models_no_out_nonlin_df |>
    ggplot(aes(x = optim_fitted_glm, y = optim_residuals_glm_dev)) +
    geom_point(color = vir_col[20]) +
    geom_smooth(method = "loess", formula = y ~ x, se = FALSE, color = vir_col[15]) +
    geom_hline(yintercept = 0, linetype = "dashed", color = vir_col[10]) +
    labs(x = "Fitted values", y = "Deviance residuals") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

```

However, examining the deviance residuals against the predicted values shows that the model is still misspecified.

To address this, we could introduce nonlinear terms, such as squares, logarithms, or interaction terms between covariates. After trying some combinations and transformations, both straightforward and less conventional, the plot has gotten better after adding the intercation term of $T_{OR1} \times canthi4Max1$

The main issue now is the multicollinearity introduced by this term, as indicated by the VIF values below:

```{r, message=FALSE}

optim_nonlin_no_out_multicol_glm_gamma <- glm(
    formula = aveOralM ~ Ethnicity_Asian + T_atm + T_Max1 +
              T_OR1 * canthi4Max1,
    family = Gamma(link = "log"),
    data = df_dummies_no_out
)

vif(optim_nonlin_no_out_multicol_glm_gamma) |> t() |> kable()
```

Fortunately, by simply centering both variables around their means, we eliminate the correlation between them and can fit the final model, which achieves a reduced AIC of $106.69$.

```{r, message=FALSE}
vif(optim_nonlin_no_out_glm_gamma) |> t() |> kable()
summary(optim_nonlin_no_out_glm_gamma)
```

Even after removing the high-leverage extreme values, the Q–Q plot remains virtually unchanged. A likely explanation is that we are sacrificing a substantial amount of the model's predictive power by restricting ourselves to a small subset of variables in order to maintain interpretability.

```{r}
qq_resid <- data.frame(
    qq_resid = qresid(optim_nonlin_no_out_glm_gamma)
)

qq_resid |>
    ggplot() +
    stat_qq_line(aes(sample = qq_resid), color = vir_col[15], lwd = 1) +
    stat_qq(aes(sample = qq_resid), color = vir_col[10]) +
    theme(panel.border = element_rect(color = "black", linewidth = 1, fill = NA))
```

In this case, removing the extreme values is not correct, because a model with more variables or additional ways to explore the data might be able to predict these observations. This suggests that the high-leverage points are not problematic on their own - rather, our model is designed for simplicity and interpretability (variance vs bias problem).

For this reason, our final model will be trained on the dataset that includes the extreme values:

```{r}

mean_T_OR1 <- df_dummies$T_OR1 |> mean()
mean_canthi4Max1 <- df_dummies$canthi4Max1 |> mean()
mean_T_atm <- df_dummies$T_atm |> mean()
mean_T_Max1 <- df_dummies$T_Max1 |> mean()

final_df <- df_dummies |>
    mutate(
        T_OR1_c = T_OR1 - mean_T_OR1,
        canthi4Max1_c = canthi4Max1 - mean_canthi4Max1,
        T_atm_c = T_atm - mean_T_atm,
        T_Max1_c = T_Max1 - mean_T_Max1
    )

final_model <- glm(
    formula = aveOralM ~ Ethnicity_Asian + T_atm_c + T_Max1_c +
              T_OR1_c * canthi4Max1_c,
    family = Gamma(link = "log"),
    data = final_df
)

summary(final_model)
```

```{r}
#| layout-ncol: 2
#| fig-cap:
#| - "Final model deviance residuals to fitted values"
#| - "Q-Q plot of quantile residuals"

final_cooks_df <- data.frame(
    index = 1:length(cooks.distance(final_model)),
    cooks_distance = cooks.distance(final_model),
    qq_resid = qresid(final_model),
    theoretical_q = qqnorm(qresid(final_model), plot.it = FALSE)$x
)

models_final_df = data.frame(
    optim_nonlin_fitted_glm = 
        final_model$fitted.values,
    optim_nonlin_residuals_glm_dev = 
        resid(final_model, type = "deviance")
)

models_final_df |>
    ggplot(aes(x = optim_nonlin_fitted_glm, y = optim_nonlin_residuals_glm_dev)) +
    geom_point(color = vir_col[20]) +
    geom_smooth(method = "loess", formula = y ~ x, se = FALSE, color = vir_col[15]) +
    geom_hline(yintercept = 0, linetype = "dashed", color = vir_col[10]) +
    labs(x = "Fitted values", y = "Deviance residuals") +
    theme(panel.border = element_rect(color = "black", 
    linewidth = 1, fill = NA))

final_cooks_df |>
    ggplot() +
    geom_text(
    data = subset(final_cooks_df, cooks_distance > cook_threshold),
    aes(x = theoretical_q, y = qq_resid, label = index),
    vjust = -1, hjust = 1
    ) +
    stat_qq_line(aes(sample = qq_resid), color = vir_col[15], lwd = 1) +
    stat_qq(aes(sample = qq_resid), color = vir_col[10]) +
    geom_point(data = subset(final_cooks_df, cooks_distance > cook_threshold),
    aes(x = theoretical_q, y = qq_resid), 
    color = vir_col[20], size = 3,
    fill = vir_col[20], alpha = 0.6) +
    theme(panel.border = element_rect(color = "black", linewidth = 1, fill = NA))
```

```{r}
cv_results <- cv.glm(data = final_df, glmfit = final_model, K = 10)
cv_results$delta[2] |> sqrt()
```

## 3.2 Model Interpretation
The final model primarily relies on other temperature variables, with only Asian ethnicity showing a noticeable effect on the outcome. Since all final covariates are centered around zero, the intercept must be interpreted differently:

```{r}
#| echo: false

coefficients <- final_model |> coef() |> as.vector()

eff_0 = exp(coefficients[1])
eff_1 = exp(coefficients[2])
eff_2 = exp(coefficients[3]) 
eff_3 = exp(coefficients[4])
eff_4 = exp(coefficients[5])

table <- data.frame(
  Variable = c("Intercept term", "$Ethnicity_{Asian}$", "$T_{atm}-c$", "$T_{Max1}-c$"),
  Description = c("Basis temperature",
                  "Asian ethnicity on average increases the temperature", 
                  "Ambient temperature has a very small negative effect", 
                  "Maximum face temperature has the strongest effect")
)
table$Estimate <- c(
  sprintf("$exp(\\widehat{\\beta}_0) = %.2f$", eff_0),
  sprintf("$exp(\\widehat{\\beta}_1) = %.3f \\%%$", (eff_1 - 1) * 100), 
  sprintf("$exp(\\widehat{\\beta}_2) = %.3f \\%%$", (eff_2 - 1) * 100), 
  sprintf("$exp(\\widehat{\\beta}_3) = %.3f \\%%$", (eff_3 - 1) * 100)
)

kable(table, escape = FALSE)
```

The interaction term offers valuable insight into the temperature scanning. If only one region, either the mouth or the canthi, shows high temperatures, the outcome temperature does not change significantly. It is only when both regions exhibit elevated temperatures that the model registers a substantial increase, indicating a situation where the person does not simply have a single randomly hotter spot.

# 4 Other Models

Because our initial model did not achieve the quality we hoped for, we experimented with several alternative approaches, focusing mainly on tree-boosting methods such as XGBoost and LightGBM. We also tested a recently introduced transformer-based architecture for tabular data, known as TabPFN.

The tree-boosting models required hyperparameter tuning, but given the small size of our dataset, this was straightforward and took no more than a minute using a simple grid search with 5-fold cross-validation. In contrast, TabPFN works entirely out of the box and requires no tuning at all, which completely removes the need to think about optimization.

The results were surprising. Despite being far more complex, these advanced models did not achieve noticeably better performance. In the end, our simple GLM proved to be fully competitive with them.

```{r}
#| echo: false

table <- data.frame(
  Model = c("GLM", "XGBoost", "LightGBM", "TabPFN"),
  Hyperparameters = c("-", "$\\eta = 0.01$, depth = 1, bag = 0.7, feature = 0.9", 
                      "$\\eta = 0.05$, max_leaves = 31, bag = 0.8, feature = 0.9", "-"),
  RMSPE = c("0.3444", "0.345", "0.3587", "0.3416")
)

kable(table, escape = FALSE)
```

```{r}
numerical_cols <- df |> select(where(is.numeric)) |> colnames()
predictor_num_cols <- numerical_cols[numerical_cols != "aveOralM"]
predictor_cat_cols <- df |> select(where(is.factor)) |> colnames()

df_centered <- df %>%
    mutate(
        across(
            .cols = all_of(predictor_num_cols),
            .fns = ~ . - mean(.)
        ),
        
        across(
            .cols = all_of(predictor_cat_cols),
            .fns = ~ as.integer(.) - 1
        )
    )
```

```{r}
#| echo: false
#| eval: false

df_export <- df %>% 
    mutate(
        across(
            .cols = all_of(predictor_cat_cols),
            .fns = ~ as.integer(.) - 1
        )
    )
write.csv(df_export, "clean_train.csv", row.names = FALSE)
```

```{r}
#| eval: false

library(xgboost)

param_grid <- expand.grid(
    learning_rate = c(0.01, 0.05, 0.1),
    max_depth = c(1),
    feature_fraction = c(0.2, 0.5, 0.9),
    bagging_fraction = c(0.7, 0.8, 1.0)
)
all_predictor_cols <- c(predictor_cat_cols, predictor_num_cols)

X <- as.matrix(df_centered |> select(all_of(all_predictor_cols)))
y <- df_centered |> pull(aveOralM)
xgb_train <- xgb.DMatrix(data = X, label = y)
results_xgboost <- data.table()

for (i in 1:nrow(param_grid)) {
    p <- param_grid[i, ]

    params <- list(
    objective = "reg:squarederror",
    eta = p$learning_rate,
    max_depth = p$max_depth,
    subsample = p$bagging_fraction,
    colsample_bytree = p$feature_fraction
    )

    cv_results <- xgb.cv(
    params = params,
    data = xgb_train,
    nfold = 5,
    nrounds = 1500,
    metrics = "rmse",
    early_stopping_rounds = 10,
    verbose = TRUE,
    showsd = TRUE
    )

    results_xgboost <- rbind(results_xgboost, cv_results$evaluation_log[cv_results$best_iteration, ])
}

# best 0.345              0.01         1              0.9              0.7
# best 0.345              0.05         1              0.9              1.0
```

```{r}
#| eval: false
library(lightgbm)

all_predictor_cols <- c(predictor_cat_cols, predictor_num_cols)

X <- as.matrix(df_centered |> select(all_of(all_predictor_cols)))
y <- df_centered |> pull(aveOralM)

lgb_train <- lgb.Dataset(
    data = X, 
    label = y,
    categorical_feature = predictor_cat_cols
)

param_grid <- expand.grid(
    learning_rate = c(0.01, 0.05, 0.1),
    num_leaves = c(15, 31, 63),
    feature_fraction = c(0.7, 0.9),
    bagging_fraction = c(0.8, 1.0)
)

param_grid_results <- param_grid
param_grid_results$results <- rep(0, param_grid |> nrow())
param_grid_results$index <- rep(0, param_grid |> nrow())

best_score <- Inf
best_params <- list()

for (i in 1:nrow(param_grid)) {
    p <- param_grid[i, ]

    current_params <- list(
        objective = "regression",
        metric = "rmse",
        boosting = "gbdt",
        verbose = 10,
        learning_rate = p$learning_rate,
        num_leaves = p$num_leaves,
        feature_fraction = p$feature_fraction,
        bagging_fraction = p$bagging_fraction
    )

    cv_result <- lgb.cv(
        params = current_params,
        data = lgb_train,
        nfold = 5,
        nrounds = 400,
        early_stopping_rounds = 10,
        eval_freq = 1
    )
    
    min_rmse_index <- cv_result$best_iter
    current_score <- cv_result$best_score
    param_grid_results$results[i] <- current_score
    param_grid_results$index[i] <- min_rmse_index

    if (current_score < best_score) {
        best_score <- current_score
        best_params <- p
        best_params$nrounds <- min_rmse_index
    }
}

# learning_rate num_leaves feature_fraction bagging_fraction   results index
# 14          0.05         31              0.9           0.8 03587548  57
```